# Banking Transaction ETL Data Pipeline

This project implements an **end-to-end batch data engineering pipeline** for processing inter-bank payment transactions.
It demonstrates how banking transaction data can be ingested, stored, transformed, and aggregated into analytics-ready tables using **industry-standard tooling and best practices**.

The pipeline emphasizes:

* Batch correctness over real-time complexity
* Clear separation of raw, staging, and reporting layers
* Reproducibility using Infrastructure as Code and containerized orchestration
* **Production-grade patterns**: incremental processing, data quality validation, source freshness monitoring

---
## Table of Contents

- [Problem Statement](#problem-statement)
- [Overview](#overview)
- [Dataset](#dataset)
- [Architecture](#architecture)
- [High-Level Architecture](#high-level-architecture)
- [Tech Stack](#tech-stack)
- [Prerequisites](#prerequisites)
- [Quick Start Guide](#quick-start-guide)
- [Detailed Setup Instructions](#detailed-setup-instructions)
- [dbt Transformations](#dbt-transformations)
- [Data Quality & Testing](#data-quality--testing)
- [Project Structure](#project-structure)
- [Acknowledgements](#acknowledgements)

---

## Problem Statement

As a core component of the financial system, banks process large volumes of inter-bank payment transactions every day. These transactions support a wide range of economic activities, including fund transfers, settlements, and cross-border payments.

In practice, inter-bank transaction data is typically generated by multiple source systems and arrives incrementally throughout the day. Without a structured and automated data pipeline, banks risk:

* Inconsistent settlement figures
* Delayed reporting
* Poor auditability of transaction flows

This project simulates how such inter-bank payment data can be handled using a **reliable batch-oriented data pipeline**, closely reflecting real-world banking data engineering patterns.

---

### Overview

This project implements an end-to-end batch data engineering pipeline for processing inter-bank payment transactions. The pipeline ingests synthetic inter-bank transaction data containing information such as sender and receiver banks, transaction amounts, currencies, and timestamps.

The data pipeline follows a layered architecture, where raw transaction data is first stored in a data lake to preserve the original records for audit and traceability purposes. The data is then loaded into BigQuery for transformation and analysis.

Workflow orchestration is handled using Apache Airflow, enabling the pipeline to run on a scheduled basis and ensuring that each stage of the data lifecycle—ingestion, loading, transformation, and reporting—executes reliably and in the correct sequence.

The final output of the project consists of structured reporting tables and visualizations that allow users to:

- Monitor daily and monthly inter-bank transaction volumes
- Analyze settlement amounts by bank, currency, and payment format
- Support reconciliation and operational reporting needs
- Detect potential risk patterns and anomalies

## Architecture
![Architecture Diagram](./docs/Pipeline_Architecture.png)

### Dashboard 
The dashboard can be accessed via [Looker Studio](https://lookerstudio.google.com/reporting/3e5c5ca1-4f0a-4bdc-966a-fe3b4264704a)

![Executive Summary Dashboard](./docs/executive_summary.jpg)

![Operational Monitoring Dashboard](./docs/operational_monitoring.jpg)

![Risk Analytics Dashboard](./docs/risk_analytics.jpg)

---

## Dataset

* **Source:** [IBM Synthetic AML Transaction Dataset](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml)
* **File used:** `LI-Medium_Trans.csv`

**Characteristics:**

* Inter-bank transfers
* Transaction timestamps
* Bank and account identifiers
* Payment amounts and currencies
* Payment formats (ACH, wire transfer, cheque)
* AML (Anti-Money Laundering) flags

The dataset is **fully synthetic** and safe for demonstration purposes.

---

## High-Level Architecture

```
Local CSV
  ↓
Google Cloud Storage (Raw Data Lake)
  ↓
BigQuery (interbank_raw)
  ↓
dbt Transformations
  ↓
Analytics-Ready Tables
  ↓
Looker Studio Dashboards
```

---

### Orchestration Flow

```
Airflow DAG: interbank_batch_pipeline
└── ingest_to_gcs
    └── load_gcs_to_bigquery
        └── validate_data_quality
            └── run_dbt_transformations
                └── run_dbt_tests
```

---

## Tech Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| Cloud Platform | Google Cloud Platform | Infrastructure hosting |
| Data Lake | Cloud Storage | Raw data storage |
| Data Warehouse | BigQuery | Analytical queries |
| Infrastructure | Terraform | Infrastructure as Code |
| Orchestration | Apache Airflow | Workflow scheduling |
| Transformation | dbt Core | SQL-based transformations |
| Data Quality | Great Expectations | Data validation |
| Ingestion | Python | Data loading scripts |
| Containerization | Docker | Local reproducible runtime |
| Visualization | Looker Studio | Reporting dashboards |

---

## Prerequisites

Before you begin, ensure you have the following installed:

- [ ] [Docker](https://docs.docker.com/get-docker/) (v20.10+)
- [ ] [Docker Compose](https://docs.docker.com/compose/install/) (v2.0+)
- [ ] [Terraform](https://developer.hashicorp.com/terraform/downloads) (v1.3+)
- [ ] [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) (gcloud CLI)
- [ ] [Python](https://www.python.org/downloads/) (v3.9+)
- [ ] A [GCP Account](https://cloud.google.com/) with billing enabled

---

## Quick Start Guide

```bash
# 1. Clone the repository
git clone https://github.com/Sefviaaa/banking-transaction-pipeline.git
cd banking-transaction-pipeline

# 2. Download the dataset
# Download LI-Medium_Trans.csv from Kaggle and place it in: 
# ingestion/data/raw/LI-Medium_Trans.csv

# 3. Set up GCP infrastructure
cd infra/terraform
cp terraform.tfvars.example terraform.tfvars  # Edit with your project ID
terraform init
terraform apply

# 4. Configure environment
cd ../../orchestration
cp .env.example .env  # Edit with your credentials

# 5. Set up GCP credentials
mkdir -p credentials
# Place your service account JSON key as credentials/credentials.json

# 6. Set up dbt profile
mkdir -p ~/.dbt
cp ../dbt/profiles.yml.example ~/.dbt/profiles.yml  # Edit with your project ID

# 7. Start the pipeline
docker compose up -d

# 8. Access Airflow UI
open http://localhost:8080  # Login: admin / admin
```

---

## Detailed Setup Instructions

### Step 1: Clone the Repository

```bash
git clone https://github.com/Sefviaaa/banking-transaction-pipeline.git
cd banking-transaction-pipeline
```

### Step 2: Download and Prepare the Dataset

1. Go to [Kaggle - IBM Transactions for AML](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml)
2. Download `LI-Medium_Trans.csv`
3. Create the data directory and compress the file (faster uploads to GCS):

```bash
mkdir -p ingestion/data/raw
mv ~/Downloads/LI-Medium_Trans.csv ingestion/data/raw/
cd ingestion/data/raw
gzip LI-Medium_Trans.csv  # Creates LI-Medium_Trans.csv.gz (~600MB vs 3GB)
```

> **Note:** The pipeline uses the compressed `.gz` file for faster GCS uploads. BigQuery can load `.gz` files directly.

### Step 3: Set Up Google Cloud Platform

#### 3.1 Create a GCP Project

```bash
# Create a new project (or use existing)
gcloud projects create your-project-id --name="Banking Pipeline"
gcloud config set project your-project-id

# Enable required APIs
gcloud services enable bigquery.googleapis.com
gcloud services enable storage.googleapis.com
```

#### 3.2 Create a Service Account

```bash
# Create service account
gcloud iam service-accounts create banking-pipeline \
    --display-name="Banking Pipeline Service Account"

# Grant permissions
gcloud projects add-iam-policy-binding your-project-id \
    --member="serviceAccount:banking-pipeline@your-project-id.iam.gserviceaccount.com" \
    --role="roles/bigquery.admin"

gcloud projects add-iam-policy-binding your-project-id \
    --member="serviceAccount:banking-pipeline@your-project-id.iam.gserviceaccount.com" \
    --role="roles/storage.admin"

# Download credentials
gcloud iam service-accounts keys create orchestration/credentials/credentials.json \
    --iam-account=banking-pipeline@your-project-id.iam.gserviceaccount.com
```

### Step 4: Provision Infrastructure with Terraform

```bash
cd infra/terraform

# Create your variables file
cat > terraform.tfvars << EOF
project_id  = "your-project-id"
region      = "US"
bucket_name = "your-unique-bucket-name"
EOF

# Initialize and apply
terraform init
terraform apply
```

This creates:
- GCS bucket for raw data
- BigQuery datasets: `interbank_raw`, `interbank_dbt_dev`, `interbank_prod`

### Step 5: Configure Environment Variables

```bash
cd ../../orchestration

# Copy the example file
cp .env.example .env

# Edit .env with your values
nano .env  # or use your preferred editor
```

Your `.env` file should contain:

```bash
# GCP Configuration
GCP_PROJECT_ID=your-project-id
GCS_RAW_BUCKET=your-unique-bucket-name
BQ_DATASET_ID=interbank_raw
BQ_TABLE_ID=transactions
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/google/credentials.json

# Airflow Security
AIRFLOW_SECRET_KEY=your-secure-random-string-here
```

### Step 6: Set Up dbt Profile

Create a `profiles.yml` in the `dbt/` directory:

```bash
cat > dbt/profiles.yml << EOF
banking_transaction_pipeline:
  target: dev
  outputs:
    dev:
      type: bigquery
      method: service-account
      project: your-project-id
      dataset: interbank
      location: US
      threads: 4
      keyfile: /opt/airflow/google/credentials.json
EOF
```

> **Note:** The `keyfile` path matches the Docker volume mount in `docker-compose.yml`.

### Step 7: Start the Pipeline

```bash
cd orchestration

# Build and start containers
docker compose up -d

# Check logs
docker compose logs -f
```

### Step 8: Access Airflow and Trigger the Pipeline

1. Open **http://localhost:8080** in your browser
2. Login with:
   - Username: `admin`
   - Password: `admin`
3. Find the DAG: `interbank_batch_pipeline`
4. Toggle the DAG **ON**
5. Click **Trigger DAG** to run manually

### Step 9: Verify the Results

```bash
# Check BigQuery for raw data
bq query --use_legacy_sql=false \
  'SELECT COUNT(*) FROM `your-project-id.interbank_raw.transactions`'

# Check dbt transformation results
bq query --use_legacy_sql=false \
  'SELECT * FROM `your-project-id.interbank.fact_transactions_daily` LIMIT 10'
```

---

## dbt Transformations

The Airflow DAG runs **dbt Core CLI** commands directly inside the Docker container.

### What the dbt tasks do

* `dbt deps` - Installs dbt packages (dbt_utils, codegen)
* `dbt run` - Runs staging models, builds dimension/fact tables, produces reporting tables
* `dbt test` - Executes data quality tests

### Data Model Architecture

```
stg_interbank_transactions (staging)
    │
    ├── fact_transactions (granular fact table)
    │       │
    │       ├── fact_transactions_daily
    │       ├── rpt_hourly_volume
    │       ├── rpt_currency_flow
    │       ├── rpt_bank_flow_matrix
    │       ├── rpt_risk_summary
    │       └── rpt_pipeline_health
    │
    ├── dim_bank
    └── dim_currency
```

### Target Datasets

* **Development:** `interbank_dbt_dev`
* **Production:** `interbank_prod`

### Key Models
### Key Models

| Layer | Model | Description |
|-------|-------|-------------|
| Staging | `stg_interbank_transactions` | Type casting, timestamp normalization, column standardization |
| Fact | `fact_transactions` | Granular fact table with all transaction details |
| Fact | `fact_transactions_daily` | Daily aggregated transactions by bank and currency |
| Dimension | `dim_bank` | Bank reference data |
| Dimension | `dim_currency` | Currency reference data |
| Report | `rpt_hourly_volume` | Hourly transaction volume for heatmap analysis |
| Report | `rpt_currency_flow` | Currency exchange flow analysis |
| Report | `rpt_bank_flow_matrix` | Bank-to-bank transaction patterns |
| Report | `rpt_risk_summary` | AML flag rates and risk metrics |
| Report | `rpt_pipeline_health` | Pipeline monitoring metrics |

## Data Quality & Testing

This project implements a **multi-layered data quality strategy**:

### Great Expectations Validation

Before dbt transformations run, the pipeline validates raw data using Great Expectations:

| Check | Description |
|-------|-------------|
| `expect_column_values_to_not_be_null` | Critical fields (timestamp, from_bank, amount) must exist |
| `expect_column_values_to_be_positive` | Transaction amounts must be >= 0 |
| `expect_column_values_to_be_in_set` | AML flag must be 0 or 1 |
| `expect_table_row_count_to_be_between` | Table must not be empty |

If validation fails, the pipeline stops before running expensive transformations.

### dbt Tests

* `not_null` checks on critical fields
* `unique` constraints on primary keys
* Referential integrity (`relationships`)
* Custom metric sanity checks using `dbt_utils`

### Source Freshness Monitoring

Configured in `sources.yml` to detect stale data:

```yaml
freshness:
  warn_after: {count: 24, period: hour}
  error_after: {count: 48, period: hour}
```

Run `dbt source freshness` to check data recency.

### Incremental Processing

The `fact_transactions` model uses **incremental materialization** for efficiency:

```sql
{{ config(materialized='incremental', unique_key='transaction_id') }}

{% if is_incremental() %}
WHERE transaction_ts > (SELECT MAX(transaction_ts) FROM {{ this }})
{% endif %}
```

This avoids reprocessing historical data on each run, reducing compute costs at scale.

---

## Dashboards

The pipeline includes three Looker Studio dashboards for different stakeholders:

### Dashboard Overview

| Dashboard | Audience | Purpose |
|-----------|----------|---------|
| **Executive Summary** | Business Leaders | High-level KPIs, trends, and business insights |
| **Operational Monitoring** | Data Engineers | Pipeline health, data freshness, processing metrics |
| **Risk Analytics** | Risk Analysts | AML monitoring, anomaly detection, suspicious patterns |

### Data Sources for Dashboards

| Dashboard | BigQuery Tables |
|-----------|-----------------|
| Executive Summary | `fact_transactions_daily`, `rpt_currency_flow` |
| Operational Monitoring | `rpt_hourly_volume`, `rpt_pipeline_health` |
| Risk Analytics | `rpt_risk_summary`, `rpt_bank_flow_matrix` |

### Dashboard 1: Executive Summary

**Key Metrics:**
- Total Transaction Volume
- Total Amount Processed
- Average Transaction Size
- Active Banks Count

**Visualizations:**
- Daily Transaction Trend (Time Series)
- Top 10 Banks by Volume (Bar Chart)
- Transactions by Currency (Donut Chart)
- Currency Exchange Flow (Table with Heatmap)

### Dashboard 2: Operational Monitoring

**Key Metrics:**
- Last Data Refresh Timestamp
- Records Processed
- Unique Source/Destination Banks

**Visualizations:**
- Hourly Transaction Heatmap (Pivot Table)
- Daily Record Count
- Hourly Volume Distribution (Area Chart)
- Pipeline Health Weekly Summary Table

### Dashboard 3: Risk Analytics

**Key Metrics:**
- Overall Flag Rate (Gauge)
- Total Flagged Transactions
- Amount at Risk

**Visualizations:**
- Flag Rate by Payment Format (Bar Chart)
- Flag Rate by Value Tier (Bar Chart)
- Bank-to-Bank Flow Matrix (Pivot Heatmap)
- Top Suspicious Bank Pairs (Table)
  
## Project Structure
```
banking-transaction-pipeline/
├── .github/
│   └── workflows/
│       └── ci.yml                    # CI/CD pipeline (lint, test, dbt parse)
│
├── ingestion/
│   ├── data/raw/                     # Raw CSV files (not committed)
│   ├── load_to_gcs.py                # Upload to GCS
│   └── load_gcs_bq.py                # Load to BigQuery
│
├── orchestration/
│   ├── dags/
│   │   └── interbank_batch_dag.py    # Main Airflow DAG
│   ├── credentials/                  # GCP credentials (not committed)
│   ├── docker-compose.yml
│   ├── Dockerfile
│   ├── requirements.txt
│   └── .env.example
│
├── dbt/
│   ├── models/
│   │   ├── staging/                  # Data cleaning & standardization
│   │   ├── marts/                    # Facts, dimensions, reports
│   │   └── sources.yml               # Source freshness configuration
│   ├── dbt_project.yml
│   └── packages.yml
│
├── great_expectations/               # Data quality framework
│   ├── great_expectations.yml        # GX configuration
│   ├── expectations/
│   │   └── transactions_suite.json   # Validation rules
│   └── run_validation.py             # Validation runner script
│
├── tests/                            # Python unit tests
│   ├── conftest.py
│   └── test_load_to_gcs.py
│
├── infra/
│   └── terraform/                    # Infrastructure as Code
│       ├── main.tf
│       ├── variables.tf
│       └── outputs.tf
│
├── docs/
│   └── Pipeline_Architecture.png     # Architecture diagram
│
├── .gitignore
└── README.md
```

---

## Acknowledgments

- IBM for the synthetic AML transaction dataset
- The dbt, Airflow, and Terraform communities
- Google Cloud Platform for infrastructure services
