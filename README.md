# Banking Transaction Data Pipeline
---
This project implements an automated batch data pipeline that processes banking transaction data for reliable end-of-day and monthly reporting. The pipeline ingests raw transaction data, stores it in a data lake, loads it into a data warehouse, and transforms it into analytics-ready tables designed for operational and management reporting.


### Problem Statement
As a core component of the financial system, banks process large volumes of inter-bank payment transactions every day. These transactions support a wide range of economic activities, including fund transfers between financial institutions, corporate payments, and settlement of obligations across different banking channels. Due to the critical nature of these transactions, banks must ensure that payment data is processed accurately, consistently, and in a timely manner to support settlement, reconciliation, and operational reporting.

In practice, inter-bank transaction data is typically generated by multiple source systems and arrives incrementally throughout the day. Without a structured and automated data pipeline, banks risk facing issues such as delayed settlement reports, inconsistent financial figures, and difficulties in auditing transaction flows. These issues can have downstream impacts on operational efficiency, regulatory compliance, and decision-making processes.

This project is motivated by the need to simulate how banks handle inter-bank payment data using reliable batch-oriented data processing. The goal is to design and implement a data engineering pipeline that can ingest transaction data, store it in a structured and auditable manner, and transform it into analytics-ready datasets that support daily and monthly settlement reporting. By focusing on correctness, repeatability, and clear data separation, this project aims to reflect real-world data engineering challenges commonly encountered in the banking industry.

### Overview
This project implements an end-to-end batch data engineering pipeline for processing inter-bank payment transactions. The pipeline ingests synthetic inter-bank transaction data containing information such as originating and receiving banks, account identifiers, transaction timestamps, payment amounts, currencies, and payment formats (e.g., wire transfer, ACH, cheque).

The data pipeline follows a layered architecture, where raw transaction data is first stored in a data lake to preserve the original records for audit and traceability purposes. The data is then loaded into a data warehouse, where transformations are applied to clean, normalize, and aggregate the data into reporting-ready tables. These transformations support common banking use cases such as settlement summaries, reconciliation checks, and transaction volume analysis across time periods, banks, and payment formats.

Workflow orchestration is handled using Apache Airflow, enabling the pipeline to run on a scheduled basis and ensuring that each stage of the data lifecycle—ingestion, loading, transformation, and reporting—is executed in a controlled and repeatable manner. This design emphasizes reliability, data quality, and maintainability, which are critical requirements in banking data environments.

The final output of the project consists of structured reporting tables and visualizations that allow users to:

- Monitor daily and monthly inter-bank transaction volumes  
- Analyze settlement amounts by bank, currency, and payment format  
- Support reconciliation and operational reporting needs  
- Demonstrate how batch data pipelines are used in real-world banking systems  

Through this project, I aim to demonstrate the practical application of data engineering principles in a banking context, focusing on building robust and auditable batch pipelines. The project reflects how data engineers contribute to the stability and reliability of core banking operations.

### Dataset
This project uses a publicly available [IBM synthetic dataset](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/data?select=HI-Medium_Trans.csv) that simulates inter-bank payment transactions. The dataset represents money transfers between originating and receiving banks, including account identifiers, transaction timestamps, payment amounts, currencies, and payment formats such as ACH, wire, and cheque.

The data is used to model batch-oriented banking workflows, including transaction ingestion, settlement reporting, reconciliation, and aggregated financial reporting. As the dataset is synthetic, it contains no real customer information and is suitable for demonstrating data engineering pipelines in a banking context.

### Tech Stack
**Containerization Platform: Docker**
Docker is used to provide standardized and isolated execution environments for each component of the data pipeline. By packaging services such as Apache Airflow and supporting dependencies into containers, Docker ensures consistent behavior across development and deployment environments. This approach simplifies environment setup, reduces configuration issues, and improves reproducibility of the data pipeline.

**Cloud Platform: Google Cloud Platform (GCP)**
Google Cloud Platform serves as the primary cloud infrastructure for this project. GCP provides reliable, scalable, and cost-effective services that are well-suited for data-intensive workloads. The project leverages GCP to store transaction data, execute batch processing workflows, and support analytical querying in a cloud-native environment.

**Infrastructure as Code: Terraform**
Terraform is used to provision and manage cloud resources on GCP using declarative configuration files. Infrastructure components such as storage buckets, data warehouse datasets, and service configurations are defined as code, enabling repeatable deployments, version control, and consistent infrastructure management across environments.

**Workflow Orchestration: Apache Airflow**
Apache Airflow is responsible for orchestrating the batch data pipeline. Using Python-defined DAGs (Directed Acyclic Graphs), Airflow schedules and coordinates each stage of the pipeline, including data ingestion, loading, and transformation tasks. Airflow also provides monitoring, logging, and retry mechanisms, ensuring reliability and operational visibility of batch workflows.

**Data Ingestion & Processing: Python**
Python is used for data ingestion and preprocessing tasks, including reading transaction data, validating schemas, and performing basic data quality checks before loading the data into downstream systems. Python’s flexibility and extensive ecosystem make it well-suited for implementing controlled, batch-oriented data processing logic in a banking context.

**Data Transformation: dbt**
dbt (data build tool) is used to perform data transformations within the data warehouse. Using SQL-based models, dbt enables structured transformation of raw transaction data into cleaned, aggregated, and reporting-ready tables. dbt also supports testing and documentation, helping ensure data consistency, correctness, and maintainability.

**Storage & Data Warehousing: Google Cloud Storage (GCS) and BigQuery**
Google Cloud Storage acts as the data lake layer, storing raw and intermediate transaction data in an immutable and auditable format. BigQuery serves as the analytical data warehouse, where transformed data is organized into reporting tables optimized for query performance. This separation supports traceability, scalability, and efficient batch analytics.

**Visualization: Looker Studio**
Looker Studio is used to create interactive dashboards and reports based on curated data stored in BigQuery. These visualizations enable users to analyze inter-bank transaction volumes, settlement amounts, and trends across time periods, banks, currencies, and payment formats. The dashboards support operational and management reporting needs in a clear and accessible manner.


### Data Modeling


### Orchestration


### Data Quality Checks


### Dashboard


### How to Run


### Project Structure

