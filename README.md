# Banking Transaction Data Pipeline
---
This project implements an automated batch data pipeline that processes banking transaction data for reliable end-of-day and monthly reporting. The pipeline ingests raw transaction data, stores it in a data lake, loads it into a data warehouse, and transforms it into analytics-ready tables designed for operational and management reporting.


### Problem Statement
As a core component of the financial system, banks process large volumes of inter-bank payment transactions every day. These transactions support a wide range of economic activities, including fund transfers between financial institutions, corporate payments, and settlement of obligations across different banking channels. Due to the critical nature of these transactions, banks must ensure that payment data is processed accurately, consistently, and in a timely manner to support settlement, reconciliation, and operational reporting.

In practice, inter-bank transaction data is typically generated by multiple source systems and arrives incrementally throughout the day. Without a structured and automated data pipeline, banks risk facing issues such as delayed settlement reports, inconsistent financial figures, and difficulties in auditing transaction flows. These issues can have downstream impacts on operational efficiency, regulatory compliance, and decision-making processes.

This project is motivated by the need to simulate how banks handle inter-bank payment data using reliable batch-oriented data processing. The goal is to design and implement a data engineering pipeline that can ingest transaction data, store it in a structured and auditable manner, and transform it into analytics-ready datasets that support daily and monthly settlement reporting. By focusing on correctness, repeatability, and clear data separation, this project aims to reflect real-world data engineering challenges commonly encountered in the banking industry.

### Overview
This project implements an end-to-end batch data engineering pipeline for processing inter-bank payment transactions. The pipeline ingests synthetic inter-bank transaction data containing information such as originating and receiving banks, account identifiers, transaction timestamps, payment amounts, currencies, and payment formats (e.g., wire transfer, ACH, cheque).

The data pipeline follows a layered architecture, where raw transaction data is first stored in a data lake to preserve the original records for audit and traceability purposes. The data is then loaded into a data warehouse, where transformations are applied to clean, normalize, and aggregate the data into reporting-ready tables. 

Workflow orchestration is handled using Apache Airflow, enabling the pipeline to run on a scheduled basis and ensuring that each stage of the data lifecycle—ingestion, loading, transformation, and reporting—is executed in a controlled and repeatable manner. This design emphasizes reliability, data quality, and maintainability, which are critical requirements in banking data environments.

The final output of the project consists of structured reporting tables and visualizations that allow users to:

- Monitor daily and monthly inter-bank transaction volumes  
- Analyze settlement amounts by bank, currency, and payment format  
- Support reconciliation and operational reporting needs  
- Demonstrate how batch data pipelines are used in real-world banking systems  

### Dataset
* Source: IBM Synthetic AML Transaction Dataset ([Kaggle](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/data?select=HI-Medium_Trans.csv))
* File used: `LI-Medium_Trans.csv`
* Characteristics:

  * Interbank transfers
  * Timestamps
  * Bank & account identifiers
  * Amounts & currencies
  * Payment formats

This dataset is **fully synthetic** and safe for demonstration purposes.

### Tech Stack
**Containerization Platform: Docker**
Docker is used to provide standardized and isolated execution environments for each component of the data pipeline. By packaging services such as Apache Airflow and supporting dependencies into containers, Docker ensures consistent behavior across development and deployment environments. This approach simplifies environment setup, reduces configuration issues, and improves reproducibility of the data pipeline.

**Cloud Platform: Google Cloud Platform (GCP)**
Google Cloud Platform serves as the primary cloud infrastructure for this project. GCP provides reliable, scalable, and cost-effective services that are well-suited for data-intensive workloads. The project leverages GCP to store transaction data, execute batch processing workflows, and support analytical querying in a cloud-native environment.

**Infrastructure as Code: Terraform**
Terraform is used to provision and manage cloud resources on GCP using declarative configuration files. Infrastructure components such as storage buckets, data warehouse datasets, and service configurations are defined as code, enabling repeatable deployments, version control, and consistent infrastructure management across environments.

**Workflow Orchestration: Apache Airflow**
Apache Airflow is responsible for orchestrating the batch data pipeline. Using Python-defined DAGs (Directed Acyclic Graphs), Airflow schedules and coordinates each stage of the pipeline, including data ingestion, loading, and transformation tasks. Airflow also provides monitoring, logging, and retry mechanisms, ensuring reliability and operational visibility of batch workflows.

**Data Ingestion & Processing: Python**
Python is used for data ingestion and preprocessing tasks, including reading transaction data, validating schemas, and performing basic data quality checks before loading the data into downstream systems. Python’s flexibility and extensive ecosystem make it well-suited for implementing controlled, batch-oriented data processing logic in a banking context.

**Data Transformation: dbt**
dbt (data build tool) is used to perform data transformations within the data warehouse. Using SQL-based models, dbt enables structured transformation of raw transaction data into cleaned, aggregated, and reporting-ready tables. dbt also supports testing and documentation, helping ensure data consistency, correctness, and maintainability.

**Storage & Data Warehousing: Google Cloud Storage (GCS) and BigQuery**
Google Cloud Storage acts as the data lake layer, storing raw and intermediate transaction data in an immutable and auditable format. BigQuery serves as the analytical data warehouse, where transformed data is organized into reporting tables optimized for query performance. This separation supports traceability, scalability, and efficient batch analytics.

**Visualization: Looker Studio**
Looker Studio is used to create interactive dashboards and reports based on curated data stored in BigQuery. These visualizations enable users to analyze inter-bank transaction volumes, settlement amounts, and trends across time periods, banks, currencies, and payment formats. The dashboards support operational and management reporting needs in a clear and accessible manner.

## High-Level Architecture

```
CSV (Local)
  ↓
Google Cloud Storage (Raw Layer)
  ↓
BigQuery (interbank_raw)
  ↓
dbt (Staging & Marts)
  ↓
Analytics-Ready Tables
```

### Orchestration Flow

```
Airflow DAG
└── ingest_to_gcs
    └── load_gcs_to_bigquery
        └── trigger_dbt_cloud_job
```

Project Structure

```
banking-transaction-pipeline/
├── ingestion/
│   ├── load_to_gcs.py
│   └── load_gcs_bq.py
│
├── orchestration/
│   ├── dags/
│   │   └── interbank_batch_dag.py
│   ├── docker-compose.yml
│   └── credentials/
│       └── credentials.json   # NOT committed
│
├── dbt/
│   ├── models/
│   │   ├── staging/
│   │   └── marts/
│   └── dbt_project.yml
│
├── infra/
│   └── terraform/
│       ├── main.tf
│       ├── variables.tf
│       └── outputs.tf
│
└── README.md
```
## Prerequisites
## Infrastructure (Terraform)

Terraform is used to **declare and manage cloud infrastructure**, ensuring the project can be recreated consistently.

### Managed Resources

* Google Cloud Storage bucket (raw data lake)
* BigQuery datasets:

  * `interbank_raw` – raw ingestion layer
  * `interbank_dbt_dev` – dbt development / staging
  * `interbank_prod` – production marts

> Terraform **does NOT run pipelines**.
> It only guarantees that required infrastructure exists.

### Apply Infrastructure

```bash
cd infra/terraform
terraform init
terraform apply
```

---

## Credentials & Environment Variables

### Google Cloud Credentials

Create a GCP Service Account with:

* BigQuery Admin (or scoped equivalent)
* Storage Object Admin

Download the JSON key and place it here:

```
orchestration/credentials/credentials.json
```

Airflow containers mount this file internally.

---

### Required Environment Variables

These **must be set before running Airflow**:

```bash
# GCP
GOOGLE_APPLICATION_CREDENTIALS=/opt/airflow/google/credentials.json
GCP_PROJECT_ID=your-gcp-project-id
GCS_BUCKET=banking-datalake

# dbt Cloud
DBT_CLOUD_ACCOUNT_ID=your_account_id
DBT_CLOUD_API_TOKEN=your_api_token
DBT_CLOUD_JOB_ID=your_job_id
```

They are consumed by:

* Python ingestion scripts
* Airflow DAG (`Variable.get` / env access)
* dbt Cloud API trigger

## How to Reproduce
Running the Pipeline Locally

### 1. Start Airflow (Docker)

```bash
cd orchestration
docker compose up
```

Services started:

* PostgreSQL (Airflow metadata DB)
* Airflow Webserver
* Airflow Scheduler

Airflow UI:

```
http://localhost:8080
```

---

### 2. Trigger the Pipeline

In Airflow UI:

* Enable DAG: `interbank_batch_pipeline`
* Trigger manually or wait for schedule

Pipeline steps:

1. Upload CSV to GCS
2. Load data into BigQuery (`interbank_raw.transactions`)
3. Trigger dbt Cloud job

---

## dbt Transformations

### Staging Layer

* Type casting
* Timestamp normalization
* Column standardization

### Dimension Tables

* `dim_bank`
* `dim_currency`

### Fact Table

* `fact_transactions_daily`

  * Daily transaction counts
  * Total transaction amounts
  * Grouped by bank and currency

### Report Table

* `rpt_bank_daily_volume`

  * Bank-level daily transaction volume

---

## Data Quality & Testing

Implemented using dbt tests:

* `not_null` checks
* Referential integrity (`relationships`)
* Aggregation consistency

These ensure analytical outputs remain reliable and auditable.


